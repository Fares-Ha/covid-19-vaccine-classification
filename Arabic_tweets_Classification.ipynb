{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhkW9IJGAxtO"
   },
   "source": [
    "# Step [1]: Prepare libraries and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxgA7cGZAxtQ"
   },
   "source": [
    "## [1.1] Include important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TgpntlYMAxtQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import matplotlib\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0Q4fWNzAxtS"
   },
   "source": [
    "## [1.2] Download data\n",
    "Arabic opinions tweets collected from twitter on covid-19 vaccination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khqxgGhpAxtS",
    "outputId": "e33045e2-677d-4223-e06e-78bdb4916de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-18 13:24:09--  https://drive.google.com/uc?export=download&id=1KepfzAhJ7dloG8XaWQf0ovQipDHYS8aI\n",
      "Resolving drive.google.com (drive.google.com)... 172.217.13.78, 2607:f8b0:4004:808::200e\n",
      "Connecting to drive.google.com (drive.google.com)|172.217.13.78|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0o-00-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/39j57sqqkuffbd3e0t6ahrgsfcefo7cj/1639833825000/04260309330816471542/*/1KepfzAhJ7dloG8XaWQf0ovQipDHYS8aI?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2021-12-18 13:24:12--  https://doc-0o-00-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/39j57sqqkuffbd3e0t6ahrgsfcefo7cj/1639833825000/04260309330816471542/*/1KepfzAhJ7dloG8XaWQf0ovQipDHYS8aI?e=download\n",
      "Resolving doc-0o-00-docs.googleusercontent.com (doc-0o-00-docs.googleusercontent.com)... 142.251.45.97, 2607:f8b0:4004:83f::2001\n",
      "Connecting to doc-0o-00-docs.googleusercontent.com (doc-0o-00-docs.googleusercontent.com)|142.251.45.97|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2962196 (2.8M) [application/zip]\n",
      "Saving to: ‘final_data.zip’\n",
      "\n",
      "final_data.zip      100%[===================>]   2.82M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2021-12-18 13:24:13 (42.5 MB/s) - ‘final_data.zip’ saved [2962196/2962196]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://drive.google.com/uc?export=download&id=1KepfzAhJ7dloG8XaWQf0ovQipDHYS8aI' -O 'final_data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8lal-N_AxtT",
    "outputId": "33e372a9-eb98-4577-d292-399c47f62718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  final_data.zip\n",
      "  inflating: test.csv                \n",
      "  inflating: train.csv               \n",
      "  inflating: valid.csv               \n"
     ]
    }
   ],
   "source": [
    "!unzip final_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kdnxMtzAxtU"
   },
   "source": [
    "## [1.3] read data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "kKTl9zp5AxtV",
    "outputId": "db369f3d-09e6-4133-9d63-20cb1fe2f6ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "valid = pd.read_csv(\"valid.csv\")\n",
    "train['tweet'][4597]=\" \"\n",
    "train['tweet'][4597]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN8GBDfPAxtX"
   },
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-iyoENK-AxtX",
    "outputId": "7ec9dc67-1db9-4cb8-de24-2017470a544a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5567"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1 = []\n",
    "y_train = []\n",
    "test1 = []\n",
    "y_test = []\n",
    "valid1 = []\n",
    "y_valid = []\n",
    "\n",
    "for i in  range(len(train)):\n",
    "    if train['label'][i] == 0:\n",
    "        y_train.append(train['label'][i])\n",
    "        train1.append(train['tweet'][i])\n",
    "    if train['label'][i] == 1:\n",
    "        y_train.append(train['label'][i])\n",
    "        train1.append(train['tweet'][i])\n",
    "for i in range(len(test)):\n",
    "    if test['label'][i] == 0:\n",
    "        y_test.append(test['label'][i])\n",
    "        test1.append(test['tweet'][i])\n",
    "    if test['label'][i] == 1:\n",
    "        y_test.append(test['label'][i])\n",
    "        test1.append(test['tweet'][i])\n",
    "for i in range(len(valid)):\n",
    "    if valid['label'][i] == 0:\n",
    "        y_valid.append(valid['label'][i])\n",
    "        valid1.append(valid['tweet'][i])\n",
    "    if valid['label'][i] == 1:\n",
    "        y_valid.append(valid['label'][i])\n",
    "        valid1.append(valid['tweet'][i])\n",
    "count=0\n",
    "newy=[]\n",
    "for i,d in enumerate(y_train):\n",
    "    if d==1:\n",
    "        if count >2783:\n",
    "            train1.pop(i-2783)\n",
    "            continue\n",
    "        else:   \n",
    "            newy.append(d)\n",
    "            count=count+1\n",
    "    else:\n",
    "        newy.append(d)\n",
    "# X_test =np.array(test1)\n",
    "# X_train=np.array(train1)\n",
    "# X_valid=np.array(valid1)\n",
    "y_train=newy\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KSGgbg_NXYC"
   },
   "source": [
    "# perprocessing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3d9xKEnhAxtY"
   },
   "outputs": [],
   "source": [
    "def delinks_mintions(data,work=1):\n",
    "    text = []\n",
    "    if work==1:\n",
    "        for d in data:\n",
    "            text.append(re.sub(r'http\\S+|@[A-Za-z0-9]+|[A-Za-z0-9]+|[A-Za-z0-9]', '', d))\n",
    "    if work == 0:\n",
    "        text.append(data)\n",
    "        return data\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SeNanfbFAxtY"
   },
   "outputs": [],
   "source": [
    "def derepeted(data,work=1):\n",
    "    text = []\n",
    "    if work==1:\n",
    "\n",
    "        for d in data:\n",
    "            text.append( re.sub(r\"(.)\\1+\",r\"\\1\",d))\n",
    "    if work == 0:\n",
    "        text.append(data)\n",
    "        return data\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2QOKOnzsAxtZ"
   },
   "outputs": [],
   "source": [
    "def standardization_of_words(words,replace,tweets,work=1):\n",
    "    if work ==1 :\n",
    "        for i in range(0, len(words)):\n",
    "            tweets = tweets.replace(words[i], replace[i])\n",
    "    return tweets\n",
    "def character_standardization(data,work=1):\n",
    "    text = []\n",
    "    if work ==1 :\n",
    "        letter = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ي\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "        replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ى\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! '] \n",
    "        for d in data:\n",
    "            tweet = re.sub(r'[\\u0617-\\u061A\\u064B-\\u0652]',\"\", d)\n",
    "            tweet=standardization_of_words(letter,replace,d)\n",
    "            text.append(tweet)\n",
    "    if work == 0:\n",
    "        text.append(data)\n",
    "        return data\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "W68klCabAxtZ"
   },
   "outputs": [],
   "source": [
    "def dedigits(data,work=1):\n",
    "    text = []\n",
    "    if work==1:\n",
    "        \n",
    "        for d in data:\n",
    "            text.append( re.sub(r\"\\d+\",\"\",d))\n",
    "    if work == 0:\n",
    "        text.append(data)\n",
    "        return data\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9NVSmJlLAxta"
   },
   "outputs": [],
   "source": [
    "def denoncharachters(data,work=1):\n",
    "    text = []\n",
    "    if work==1:\n",
    "        \n",
    "        for d in data:\n",
    "            text.append( re.sub(r\"\\W\",\" \",d))\n",
    "    if work == 0:\n",
    "        text.append(data)\n",
    "        return data\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WLPrjjnkAxta"
   },
   "outputs": [],
   "source": [
    "def deEmoji(data,work=1):\n",
    "    text = []\n",
    "    if work==1:\n",
    "        \n",
    "        for d in data:\n",
    "            needed_words= []\n",
    "            regrex_pattern = re.compile(pattern = \"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   \"]+\", flags = re.UNICODE)\n",
    "            text.append(regrex_pattern.sub(r'',d))\n",
    "    if work == 0:\n",
    "        text.append(data)\n",
    "        return data\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Xyrbi1a4Axta"
   },
   "outputs": [],
   "source": [
    "# nltk.download()\n",
    "stopwords_list = nltk.corpus.stopwords.words('arabic')\n",
    "y= 'كل كلمة كم منذ سنة'\n",
    "\n",
    "def destopwords(data,work=1):\n",
    "    text = []\n",
    "    if work==1:\n",
    "        stopwords_list = nltk.corpus.stopwords.words('arabic')\n",
    "        \n",
    "        for d in data:\n",
    "            d1=d.split()\n",
    "            needed_words= []\n",
    "            for w in d1:\n",
    "                if w not in (stopwords_list):\n",
    "                    needed_words.append(w)\n",
    "            filtered_sentence = \" \".join(needed_words)\n",
    "            text.append( filtered_sentence)\n",
    "    if work == 0:\n",
    "        text.append(data)\n",
    "        return data\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nmNNhwY4Axtb"
   },
   "outputs": [],
   "source": [
    "def denoncharachters(data,work=1):\n",
    "    text = []\n",
    "    if work==1:\n",
    "        \n",
    "        for d in data:\n",
    "            t1=re.sub(r\"\\W\",\" \",d)\n",
    "            t1=re.sub(r\"[A-Za-z]\",\" \",t1)\n",
    "            text.append( re.sub(r\"\\s+\",\" \",t1))\n",
    "    if work == 0:\n",
    "        text.append(data)\n",
    "        return data\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "HAluzx9oAxtb"
   },
   "outputs": [],
   "source": [
    "def stemming(tweet,work=1):\n",
    "    \n",
    "    result = []\n",
    "    text = []\n",
    "    words = ''\n",
    "    if work ==1 :\n",
    "        for d in tweet:\n",
    "            result = []\n",
    "            words = d.split()\n",
    "            stemmer = ISRIStemmer()\n",
    "            for word in words:\n",
    "                word = stemmer.norm(word, num=1) \n",
    "                if word.startswith('#'):\n",
    "                    result.append(word)\n",
    "                    continue\n",
    "                if not word in stemmer.stop_words:    \n",
    "                    word = stemmer.pre32(word)         \n",
    "                    word = stemmer.suf32(word)        \n",
    "                result.append(word)\n",
    "        \n",
    "            re = ' '.join(result)\n",
    "            text.append(re)\n",
    "    \n",
    "    if work == 0:\n",
    "        text.append(tweet)\n",
    "        return tweet\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "IWpO3a9uAxtb"
   },
   "outputs": [],
   "source": [
    "def standardization_of_words(words,replace,tweets,work=1):\n",
    "    if work ==1 :\n",
    "        for i in range(0, len(words)):\n",
    "            tweets = tweets.replace(words[i], replace[i])\n",
    "    return tweets\n",
    "def character_standardization(tweet,work=1):\n",
    "    text = []\n",
    "    if work == 0:\n",
    "        text.append(tweet)\n",
    "        return tweet\n",
    "    if work ==1 :\n",
    "        letter = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ي\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "        replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ى\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
    "        for d in tweet:\n",
    "            d = re.sub(r'[\\u0617-\\u061A\\u064B-\\u0652]',\"\", d)\n",
    "            d=standardization_of_words(letter,replace,d)\n",
    "            text.append(d)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "iOqzkO-uAxtc"
   },
   "outputs": [],
   "source": [
    "def preprocess(tweet, flags=[0,0,1,0,0,0,1,0]):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "        flags: list of \"work\" values for all functions will called here.\n",
    "    Output:\n",
    "        cleaned_tweet: tweet after apply all cleaning and normlizaing functions\n",
    "\n",
    "    \"\"\"\n",
    "    tweet = delinks_mintions(tweet, flags[0])\n",
    "    tweet = derepeted(tweet, flags[1])\n",
    "    tweet = dedigits(tweet, flags[2])\n",
    "    tweet = denoncharachters(tweet, flags[3])\n",
    "    tweet = deEmoji(tweet, flags[4])\n",
    "    tweet = destopwords(tweet, flags[5])\n",
    "    tweet = character_standardization(tweet, flags[6])\n",
    "    tweet = stemming(tweet, flags[7])\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3q2RQ-doAxtc"
   },
   "source": [
    "## [1.4] Prapere The Comparison Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "63zy9cQFAxtc"
   },
   "outputs": [],
   "source": [
    "model_comparison_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9Ez86M7jAxtc"
   },
   "outputs": [],
   "source": [
    "model_comparison_table['model_name'] = []\n",
    "model_comparison_table['preprocessing_methods'] = []\n",
    "model_comparison_table['accuracy'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTXHtxfhAxtd"
   },
   "source": [
    "# Step [2]: Build Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vNx8yCtAxtd"
   },
   "source": [
    "## [2.1] Extract Bag of Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xMYpIBJAxtd",
    "outputId": "33763323-8563-4b75-d67c-3d52a14eebba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5567x33145 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 131735 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# code to be written with students in class\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train1)\n",
    "X_train = vectorizer.transform(train1)\n",
    "X_valid = vectorizer.transform(valid1)\n",
    "X_test  = vectorizer.transform(test1)\n",
    "# X_test =np.array(X_test)\n",
    "# X_train=np.array(X_train)\n",
    "# X_valid=np.array(X_valid)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjq9kFncAxtd"
   },
   "source": [
    "## [2.2] Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRPsNWFaAxte",
    "outputId": "d8ee0c44-c478-4d3f-8816-4dafd54d0147"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRSRd6gNAxte"
   },
   "source": [
    "## [2.3] Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XkbykDsAxte",
    "outputId": "e6101389-8a2b-431f-c8db-87f608df9328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5804195804195804\n"
     ]
    }
   ],
   "source": [
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwJ8mXmrAxte"
   },
   "source": [
    "في المثال التالي طريقة إدخال وكتابة اسم النموذج والعمليات وإدخال الدقة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7GHESDxAxte",
    "outputId": "a4091fa6-cb32-4de2-e6ed-d57f48d5c1f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804],\n",
       " 'model_name': ['logistic regression with bag of words'],\n",
       " 'preprocessing_methods': ['none']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"none\")\n",
    "model_comparison_table['accuracy'].append(score)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IoUD3gaAxtf"
   },
   "source": [
    "# Step [3]: Build model with preprocessing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8kTJGEU0oks"
   },
   "source": [
    "## Step[3.1] removing mentions and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7Qm8B06Axtf",
    "outputId": "9e3d2879-72b9-4e64-a645-812e29b28973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5782112624217888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "train_after_pro = preprocess(train1,[1,0,0,0,0,0,0,0])\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_after_pro)\n",
    "\n",
    "X_train = vectorizer.transform(train_after_pro)\n",
    "X_valid = vectorizer.transform(valid1)\n",
    "X_test  = vectorizer.transform(test1)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYxmcv6wuo-f",
    "outputId": "eb4291c8-269c-4b47-dce0-eaa9674b8cc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804, 0.5782112624217888],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words'],\n",
       " 'preprocessing_methods': ['none', 'removing mentions and urls']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing mentions and urls\")\n",
    "model_comparison_table['accuracy'].append(score)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNqdnRFT0xQB"
   },
   "source": [
    "## Step[3.2] removing deprecated letters from a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rS53rVJfAxti",
    "outputId": "96e90343-01ca-4e6d-fc42-ab6b1c3a8df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5940375414059624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "train_after_pro = preprocess(train1,[0,1,0,0,0,0,0,0])\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_after_pro)\n",
    "\n",
    "X_train = vectorizer.transform(train_after_pro)\n",
    "X_valid = vectorizer.transform(valid1)\n",
    "X_test  = vectorizer.transform(test1)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLtoxhbjAxti",
    "outputId": "80a6ab5c-ddf2-4ba7-ce35-a058d4b0f135"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804, 0.5782112624217888, 0.5940375414059624],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing deprecated letters from a word\")\n",
    "model_comparison_table['accuracy'].append(score)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CjJyCAH0_Ln"
   },
   "source": [
    "## Step[3.3] removing digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uf5p9TzzAxtj",
    "outputId": "e47eb515-1688-448d-ed0b-a470139294ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5807876334192124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "train_after_pro = preprocess(train1,[0,0,1,0,0,0,0,0])\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_after_pro)\n",
    "\n",
    "X_train = vectorizer.transform(train_after_pro)\n",
    "X_valid = vectorizer.transform(valid1)\n",
    "X_test  = vectorizer.transform(test1)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjBdWOUvAxtk",
    "outputId": "ed40312b-22af-4124-d391-c04b54fa08f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804,\n",
       "  0.5782112624217888,\n",
       "  0.5940375414059624,\n",
       "  0.5807876334192124],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word',\n",
       "  'removing digits']}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing digits\")\n",
    "model_comparison_table['accuracy'].append(score)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuHLhKnP1KRW"
   },
   "source": [
    "## Step[3.4] removing non characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEPRiOhgAxtl",
    "outputId": "f17fee1a-bd51-4274-dfa9-d2e0e4db9326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5774751564225249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "train_after_pro = preprocess(train1,[0,0,0,1,0,0,0,0])\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_after_pro)\n",
    "\n",
    "X_train = vectorizer.transform(train_after_pro)\n",
    "X_valid = vectorizer.transform(valid1)\n",
    "X_test  = vectorizer.transform(test1)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ASOs_VlAxtm",
    "outputId": "ede05c89-c993-44a3-b0e4-26936669a4bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804,\n",
       "  0.5782112624217888,\n",
       "  0.5940375414059624,\n",
       "  0.5807876334192124,\n",
       "  0.5774751564225249],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word',\n",
       "  'removing digits',\n",
       "  'removing non characters']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing non characters\")\n",
    "model_comparison_table['accuracy'].append(score)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5-XOl2i1TFB"
   },
   "source": [
    "## Step[3.5] removing emojies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8Q3w9-_Axtn",
    "outputId": "2625dd02-8d37-43bd-fba3-002cf0f17327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5785793154214207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "train_after_pro = preprocess(train1,[0,0,0,0,1,0,0,0])\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_after_pro)\n",
    "\n",
    "X_train = vectorizer.transform(train_after_pro)\n",
    "X_valid = vectorizer.transform(valid1)\n",
    "X_test  = vectorizer.transform(test1)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejqFo9hKAxtn",
    "outputId": "50cb1ef0-9da1-4837-97a3-b81fc6c12bc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804,\n",
       "  0.5782112624217888,\n",
       "  0.5940375414059624,\n",
       "  0.5807876334192124,\n",
       "  0.5774751564225249,\n",
       "  0.5785793154214207],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word',\n",
       "  'removing digits',\n",
       "  'removing non characters',\n",
       "  'removing emojies']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing emojies\")\n",
    "model_comparison_table['accuracy'].append(score)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDOFfYkA1oNY"
   },
   "source": [
    "## Step[3.6] removing stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "se-KGPjhAxtn",
    "outputId": "ea58a2b6-fd66-481a-a038-e2f2620f080d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4994479205005521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "train_after_pro = preprocess(train1,[0,0,0,0,0,1,0,0])\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_after_pro)\n",
    "\n",
    "X_train = vectorizer.transform(train_after_pro)\n",
    "X_valid = vectorizer.transform(valid1)\n",
    "X_test  = vectorizer.transform(test1)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aov1-uJMAxto",
    "outputId": "9a607c7c-2508-428b-cb8a-b614da222a17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804,\n",
       "  0.5782112624217888,\n",
       "  0.5940375414059624,\n",
       "  0.5807876334192124,\n",
       "  0.5774751564225249,\n",
       "  0.5785793154214207,\n",
       "  0.4994479205005521],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word',\n",
       "  'removing digits',\n",
       "  'removing non characters',\n",
       "  'removing emojies',\n",
       "  'removing stop word']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing stop word\")\n",
    "model_comparison_table['accuracy'].append(score)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ckd7hy_01ukX"
   },
   "source": [
    "## Step[3.7] use character standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cpL3IxZAxto",
    "outputId": "6fa56e81-568e-414e-bad8-a6d064add2df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5311004784688995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "train_after_pro = preprocess(train1,[0,0,0,0,0,0,1,0])\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_after_pro)\n",
    "\n",
    "X_train = vectorizer.transform(train_after_pro)\n",
    "X_valid = vectorizer.transform(valid1)\n",
    "X_test  = vectorizer.transform(test1)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZTmQA0SAxtp",
    "outputId": "2dd474fa-8ef4-4319-ba8d-588d672783a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804,\n",
       "  0.5782112624217888,\n",
       "  0.5940375414059624,\n",
       "  0.5807876334192124,\n",
       "  0.5774751564225249,\n",
       "  0.5785793154214207,\n",
       "  0.4994479205005521,\n",
       "  0.5311004784688995],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word',\n",
       "  'removing digits',\n",
       "  'removing non characters',\n",
       "  'removing emojies',\n",
       "  'removing stop word',\n",
       "  'character standardization']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"character standardization\")\n",
    "model_comparison_table['accuracy'].append(score)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIX8dn9y11LJ"
   },
   "source": [
    "## Step[3.8] stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMrdSZKVAxtp",
    "outputId": "299bf0eb-ba16-4ff0-e361-9b1ad022ebd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5811556864188443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "train_after_pro = preprocess(train1,[0,0,0,0,0,0,0,1])\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_after_pro)\n",
    "\n",
    "X_train = vectorizer.transform(train_after_pro)\n",
    "X_valid = vectorizer.transform(valid1)\n",
    "X_test  = vectorizer.transform(test1)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_N9JwF5Axtp",
    "outputId": "252e5fdc-b087-42a8-967e-7a84c002f70d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804,\n",
       "  0.5782112624217888,\n",
       "  0.5940375414059624,\n",
       "  0.5807876334192124,\n",
       "  0.5774751564225249,\n",
       "  0.5785793154214207,\n",
       "  0.4994479205005521,\n",
       "  0.5311004784688995,\n",
       "  0.5811556864188443],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word',\n",
       "  'removing digits',\n",
       "  'removing non characters',\n",
       "  'removing emojies',\n",
       "  'removing stop word',\n",
       "  'character standardization',\n",
       "  'after stemming']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"logistic regression with bag of words\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"after stemming\")\n",
    "model_comparison_table['accuracy'].append(score)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJpaxvmW17Lj"
   },
   "source": [
    "## Step[3.9] removing deprecated letters from a word,removing digits,stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjYHj0p4Axtq",
    "outputId": "47a78771-18b7-44e7-ecad-3acb96b74bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5925653294074347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "train_after_pro = preprocess(train1,[0,1,1,0,0,0,0,0,1])\n",
    "# X_test=np.array(test1)\n",
    "# train_after_pro=np.array(train_after_pro)\n",
    "# X_valid=np.array(valid1)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_after_pro)\n",
    "\n",
    "X_train = vectorizer.transform(train_after_pro)\n",
    "X_valid = vectorizer.transform(valid1)\n",
    "X_test  = vectorizer.transform(test1)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nX_nBk2kB3xH",
    "outputId": "c41425f3-a457-47e7-cbeb-dc4b199dba46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804,\n",
       "  0.5782112624217888,\n",
       "  0.5940375414059624,\n",
       "  0.5807876334192124,\n",
       "  0.5774751564225249,\n",
       "  0.5785793154214207,\n",
       "  0.4994479205005521,\n",
       "  0.5311004784688995,\n",
       "  0.5811556864188443,\n",
       "  0.5925653294074347],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'Logistic regression'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word',\n",
       "  'removing digits',\n",
       "  'removing non characters',\n",
       "  'removing emojies',\n",
       "  'removing stop word',\n",
       "  'character standardization',\n",
       "  'after stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"Logistic regression\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing deprecated letters from a word,removing digits,stemming\")\n",
    "model_comparison_table['accuracy'].append(score)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwwNc73DAxtq"
   },
   "source": [
    "# Logistic Regression with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "m-Bn6gC4Axtr"
   },
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=1, max_df=0.10, ngram_range=(1, 4))\n",
    "\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer\n",
    "\n",
    "\n",
    "train_after_pro = preprocess(train1,[0,1,1,0,0,0,0,0,1])\n",
    "X_train_tfidf, tfidf_vectorizer = tfidf(train_after_pro)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test1)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(valid1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "hsgCsiOZAxtr"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_predicted = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "DSm6skXvAxtr"
   },
   "outputs": [],
   "source": [
    "# accuracy_score(y_valid, y_predicted)\n",
    "score = clf.score(X_test_tfidf,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scReUaeMBm8a",
    "outputId": "ad1812ba-67c7-4a4d-9113-9bd34410cf5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804,\n",
       "  0.5782112624217888,\n",
       "  0.5940375414059624,\n",
       "  0.5807876334192124,\n",
       "  0.5774751564225249,\n",
       "  0.5785793154214207,\n",
       "  0.4994479205005521,\n",
       "  0.5311004784688995,\n",
       "  0.5811556864188443,\n",
       "  0.5925653294074347,\n",
       "  0.6209054103790946],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'Logistic regression',\n",
       "  'tfidf with logistic regression'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word',\n",
       "  'removing digits',\n",
       "  'removing non characters',\n",
       "  'removing emojies',\n",
       "  'removing stop word',\n",
       "  'character standardization',\n",
       "  'after stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming']}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"tfidf with logistic regression\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing deprecated letters from a word,removing digits,stemming\")\n",
    "model_comparison_table['accuracy'].append(score)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZcM-VtSzApm"
   },
   "source": [
    "# MultinomialNB with Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzowxfxLAxts",
    "outputId": "556a1942-fd12-4379-fcf8-56354d49461d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6345233713654767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('RUS', RandomOverSampler()),\n",
    "                    ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(train1, y_train)\n",
    "y_pred = text_clf.predict(test1)\n",
    "score = text_clf.score(test1, y_test)\n",
    "print('Score:',text_clf.score(test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5zSJJMOBVSh",
    "outputId": "4e457a83-2f18-42f1-a42e-bf0fb2005eac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804,\n",
       "  0.5782112624217888,\n",
       "  0.5940375414059624,\n",
       "  0.5807876334192124,\n",
       "  0.5774751564225249,\n",
       "  0.5785793154214207,\n",
       "  0.4994479205005521,\n",
       "  0.5311004784688995,\n",
       "  0.5811556864188443,\n",
       "  0.5925653294074347,\n",
       "  0.6209054103790946,\n",
       "  0.6345233713654767],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'Logistic regression',\n",
       "  'tfidf with logistic regression',\n",
       "  'MultinomialNB with TfidfTransformer'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word',\n",
       "  'removing digits',\n",
       "  'removing non characters',\n",
       "  'removing emojies',\n",
       "  'removing stop word',\n",
       "  'character standardization',\n",
       "  'after stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'None']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"MultinomialNB with TfidfTransformer\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"None\")\n",
    "model_comparison_table['accuracy'].append(score)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94-IoMpPNH-M"
   },
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "fAs9hZ_ZRxBk"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train1)\n",
    "\n",
    "X_train = vectorizer.transform(train1)\n",
    "X_valid = vectorizer.transform(valid1)\n",
    "X_test  = vectorizer.transform(test1)\n",
    "y_test=np.array(y_test)\n",
    "y_train=np.array(y_train)\n",
    "y_valid=np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULJG_6ZDfj4T",
    "outputId": "2acf9109-55c8-4d31-99cd-ef3a21d071e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "print(type(X_train_tfidf))\n",
    "\n",
    "X_train_tfidf=convert_sparse_matrix_to_sparse_tensor(X_train_tfidf)\n",
    "X_train_tfidf = tf.sparse.reorder(X_train_tfidf)\n",
    "\n",
    "X_test_tfidf=convert_sparse_matrix_to_sparse_tensor(X_test_tfidf)\n",
    "X_test_tfidf = tf.sparse.reorder(X_test_tfidf)\n",
    "\n",
    "X_valid_tfidf = convert_sparse_matrix_to_sparse_tensor(X_valid_tfidf)\n",
    "X_valid_tfidf = tf.sparse.reorder(X_valid_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkLI3G7hzaxM"
   },
   "source": [
    "## Model with tfidf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "P8-zDkhjAxts"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "input_dim = X_train_tfidf.shape[1]  # Number of features\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skZN1Wu7Axts",
    "outputId": "370caf1b-1485-4b34-ea1d-baaf629cd885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                3491000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,491,011\n",
      "Trainable params: 3,491,011\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIPdQOITAxtt",
    "outputId": "1e3db88d-f957-4e52-932f-ba21145499a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:450: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279/279 [==============================] - 11s 37ms/step - loss: 0.6927 - accuracy: 0.5121 - val_loss: 0.6864 - val_accuracy: 0.5986\n",
      "Epoch 2/30\n",
      "279/279 [==============================] - 10s 36ms/step - loss: 0.5892 - accuracy: 0.9102 - val_loss: 0.6449 - val_accuracy: 0.6510\n",
      "Epoch 3/30\n",
      "279/279 [==============================] - 11s 38ms/step - loss: 0.3173 - accuracy: 0.9634 - val_loss: 0.6527 - val_accuracy: 0.6328\n",
      "Epoch 4/30\n",
      "279/279 [==============================] - 10s 36ms/step - loss: 0.1481 - accuracy: 0.9732 - val_loss: 0.6805 - val_accuracy: 0.6250\n",
      "Epoch 5/30\n",
      "279/279 [==============================] - 10s 36ms/step - loss: 0.0866 - accuracy: 0.9777 - val_loss: 0.7019 - val_accuracy: 0.6250\n",
      "Epoch 6/30\n",
      "279/279 [==============================] - 10s 36ms/step - loss: 0.0617 - accuracy: 0.9806 - val_loss: 0.7085 - val_accuracy: 0.6310\n",
      "Epoch 7/30\n",
      "279/279 [==============================] - 11s 41ms/step - loss: 0.0482 - accuracy: 0.9828 - val_loss: 0.7513 - val_accuracy: 0.6116\n",
      "Epoch 8/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0399 - accuracy: 0.9847 - val_loss: 0.7539 - val_accuracy: 0.6272\n",
      "Epoch 9/30\n",
      "279/279 [==============================] - 10s 36ms/step - loss: 0.0348 - accuracy: 0.9853 - val_loss: 0.7659 - val_accuracy: 0.6276\n",
      "Epoch 10/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0305 - accuracy: 0.9885 - val_loss: 0.7817 - val_accuracy: 0.6298\n",
      "Epoch 11/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0278 - accuracy: 0.9896 - val_loss: 0.7961 - val_accuracy: 0.6276\n",
      "Epoch 12/30\n",
      "279/279 [==============================] - 10s 36ms/step - loss: 0.0254 - accuracy: 0.9887 - val_loss: 0.8038 - val_accuracy: 0.6243\n",
      "Epoch 13/30\n",
      "279/279 [==============================] - 10s 36ms/step - loss: 0.0235 - accuracy: 0.9907 - val_loss: 0.8300 - val_accuracy: 0.6187\n",
      "Epoch 14/30\n",
      "279/279 [==============================] - 10s 36ms/step - loss: 0.0223 - accuracy: 0.9901 - val_loss: 0.8558 - val_accuracy: 0.6068\n",
      "Epoch 15/30\n",
      "279/279 [==============================] - 10s 36ms/step - loss: 0.0211 - accuracy: 0.9901 - val_loss: 0.8535 - val_accuracy: 0.6164\n",
      "Epoch 16/30\n",
      "279/279 [==============================] - 10s 36ms/step - loss: 0.0207 - accuracy: 0.9901 - val_loss: 0.8630 - val_accuracy: 0.6176\n",
      "Epoch 17/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0194 - accuracy: 0.9912 - val_loss: 0.8706 - val_accuracy: 0.6187\n",
      "Epoch 18/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0187 - accuracy: 0.9916 - val_loss: 0.8867 - val_accuracy: 0.6161\n",
      "Epoch 19/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0184 - accuracy: 0.9901 - val_loss: 0.8786 - val_accuracy: 0.6213\n",
      "Epoch 20/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0175 - accuracy: 0.9921 - val_loss: 0.8971 - val_accuracy: 0.6179\n",
      "Epoch 21/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0172 - accuracy: 0.9908 - val_loss: 0.9048 - val_accuracy: 0.6183\n",
      "Epoch 22/30\n",
      "279/279 [==============================] - 11s 38ms/step - loss: 0.0168 - accuracy: 0.9916 - val_loss: 0.9176 - val_accuracy: 0.6135\n",
      "Epoch 23/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0166 - accuracy: 0.9917 - val_loss: 0.9105 - val_accuracy: 0.6231\n",
      "Epoch 24/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0160 - accuracy: 0.9910 - val_loss: 0.9341 - val_accuracy: 0.6142\n",
      "Epoch 25/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0158 - accuracy: 0.9912 - val_loss: 0.9477 - val_accuracy: 0.6064\n",
      "Epoch 26/30\n",
      "279/279 [==============================] - 10s 38ms/step - loss: 0.0156 - accuracy: 0.9923 - val_loss: 0.9313 - val_accuracy: 0.6228\n",
      "Epoch 27/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0150 - accuracy: 0.9917 - val_loss: 0.9414 - val_accuracy: 0.6183\n",
      "Epoch 28/30\n",
      "279/279 [==============================] - 10s 36ms/step - loss: 0.0153 - accuracy: 0.9921 - val_loss: 0.9742 - val_accuracy: 0.6057\n",
      "Epoch 29/30\n",
      "279/279 [==============================] - 10s 37ms/step - loss: 0.0149 - accuracy: 0.9917 - val_loss: 0.9835 - val_accuracy: 0.6031\n",
      "Epoch 30/30\n",
      "279/279 [==============================] - 10s 36ms/step - loss: 0.0145 - accuracy: 0.9919 - val_loss: 0.9770 - val_accuracy: 0.6086\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_tfidf, y_train,\n",
    "                     epochs=30,\n",
    "                     verbose=True,\n",
    "                     validation_data=(X_valid_tfidf, y_valid),\n",
    "                     batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYmTAmurAxtt",
    "outputId": "30a63341-ee54-41f4-ec00-4c0f043ce8b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9943\n",
      "Testing Accuracy:  0.5944\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_tfidf, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_tfidf, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDJzHRHRBFjo",
    "outputId": "c87c76fb-3d3f-43c9-f516-263be00d85a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804,\n",
       "  0.5782112624217888,\n",
       "  0.5940375414059624,\n",
       "  0.5807876334192124,\n",
       "  0.5774751564225249,\n",
       "  0.5785793154214207,\n",
       "  0.4994479205005521,\n",
       "  0.5311004784688995,\n",
       "  0.5811556864188443,\n",
       "  0.5925653294074347,\n",
       "  0.6209054103790946,\n",
       "  0.6345233713654767,\n",
       "  0.5944055914878845],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'Logistic regression',\n",
       "  'tfidf with logistic regression',\n",
       "  'MultinomialNB with TfidfTransformer',\n",
       "  'deep nueral network with tfidf'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word',\n",
       "  'removing digits',\n",
       "  'removing non characters',\n",
       "  'removing emojies',\n",
       "  'removing stop word',\n",
       "  'character standardization',\n",
       "  'after stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'None',\n",
       "  'removing deprecated letters from a word,removing digits,stemming']}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"deep nueral network with tfidf\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing deprecated letters from a word,removing digits,stemming\")\n",
    "model_comparison_table['accuracy'].append(accuracy)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFLJ-9Y_zZUu"
   },
   "source": [
    "### create a tokenizer and adding a padding to create an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "3-dxrcT_TLWH"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(preprocess(train1,[0,1,1,0,0,0,0,0,1]))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(preprocess(train1,[0,1,1,0,0,0,0,0,1]))\n",
    "X_test = tokenizer.texts_to_sequences(test1)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "2LUXycUbJrPt"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XABDksahztVb"
   },
   "source": [
    "## Model with Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k681W8-OTfgC",
    "outputId": "6bd48c6d-2364-47c1-d635-eb427f48f576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           1699350   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5000)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                50010     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,749,371\n",
      "Trainable params: 1,749,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxmNR2HiTwvY",
    "outputId": "b9d00a0a-2b89-4754-bb30-71ae277c43a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.6935 - accuracy: 0.4988 - val_loss: 0.6936 - val_accuracy: 0.3342\n",
      "Epoch 2/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.6932 - accuracy: 0.5087 - val_loss: 0.6840 - val_accuracy: 0.6654\n",
      "Epoch 3/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.6329 - accuracy: 0.6364 - val_loss: 0.6752 - val_accuracy: 0.5904\n",
      "Epoch 4/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.2821 - accuracy: 0.9048 - val_loss: 0.8375 - val_accuracy: 0.5351\n",
      "Epoch 5/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.1518 - accuracy: 0.9506 - val_loss: 0.9527 - val_accuracy: 0.5237\n",
      "Epoch 6/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0992 - accuracy: 0.9653 - val_loss: 1.0040 - val_accuracy: 0.5480\n",
      "Epoch 7/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0771 - accuracy: 0.9702 - val_loss: 1.0260 - val_accuracy: 0.5591\n",
      "Epoch 8/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.0656 - accuracy: 0.9711 - val_loss: 1.0964 - val_accuracy: 0.5598\n",
      "Epoch 9/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0553 - accuracy: 0.9745 - val_loss: 1.2594 - val_accuracy: 0.5259\n",
      "Epoch 10/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0498 - accuracy: 0.9775 - val_loss: 1.3024 - val_accuracy: 0.5318\n",
      "Epoch 11/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0453 - accuracy: 0.9772 - val_loss: 1.3354 - val_accuracy: 0.5480\n",
      "Epoch 12/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.0422 - accuracy: 0.9777 - val_loss: 1.4532 - val_accuracy: 0.5219\n",
      "Epoch 13/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0400 - accuracy: 0.9793 - val_loss: 1.4204 - val_accuracy: 0.5488\n",
      "Epoch 14/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0345 - accuracy: 0.9797 - val_loss: 1.5097 - val_accuracy: 0.5661\n",
      "Epoch 15/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0348 - accuracy: 0.9815 - val_loss: 1.6243 - val_accuracy: 0.5296\n",
      "Epoch 16/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0344 - accuracy: 0.9801 - val_loss: 1.4908 - val_accuracy: 0.5745\n",
      "Epoch 17/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0333 - accuracy: 0.9801 - val_loss: 1.6610 - val_accuracy: 0.5513\n",
      "Epoch 18/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.0317 - accuracy: 0.9802 - val_loss: 1.5118 - val_accuracy: 0.5672\n",
      "Epoch 19/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0312 - accuracy: 0.9813 - val_loss: 1.7071 - val_accuracy: 0.5473\n",
      "Epoch 20/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0294 - accuracy: 0.9817 - val_loss: 1.7355 - val_accuracy: 0.5605\n",
      "Epoch 21/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0297 - accuracy: 0.9811 - val_loss: 1.9753 - val_accuracy: 0.5175\n",
      "Epoch 22/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.0285 - accuracy: 0.9822 - val_loss: 1.8357 - val_accuracy: 0.5524\n",
      "Epoch 23/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0289 - accuracy: 0.9815 - val_loss: 1.7948 - val_accuracy: 0.5547\n",
      "Epoch 24/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0289 - accuracy: 0.9826 - val_loss: 1.9003 - val_accuracy: 0.5686\n",
      "Epoch 25/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0286 - accuracy: 0.9826 - val_loss: 1.9383 - val_accuracy: 0.5801\n",
      "Epoch 26/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0278 - accuracy: 0.9819 - val_loss: 2.1181 - val_accuracy: 0.5488\n",
      "Epoch 27/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0290 - accuracy: 0.9826 - val_loss: 2.0072 - val_accuracy: 0.5616\n",
      "Epoch 28/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0269 - accuracy: 0.9824 - val_loss: 2.0958 - val_accuracy: 0.5569\n",
      "Epoch 29/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0262 - accuracy: 0.9842 - val_loss: 2.2432 - val_accuracy: 0.5377\n",
      "Epoch 30/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0290 - accuracy: 0.9824 - val_loss: 2.0256 - val_accuracy: 0.5598\n",
      "Training Accuracy: 0.9840\n",
      "Testing Accuracy:  0.5598\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=30,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=20)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEeYYqN7A_il",
    "outputId": "1e3488cc-c182-4455-f44e-08215378f2e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804,\n",
       "  0.5782112624217888,\n",
       "  0.5940375414059624,\n",
       "  0.5807876334192124,\n",
       "  0.5774751564225249,\n",
       "  0.5785793154214207,\n",
       "  0.4994479205005521,\n",
       "  0.5311004784688995,\n",
       "  0.5811556864188443,\n",
       "  0.5925653294074347,\n",
       "  0.6209054103790946,\n",
       "  0.6345233713654767,\n",
       "  0.5944055914878845,\n",
       "  0.559808611869812],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'Logistic regression',\n",
       "  'tfidf with logistic regression',\n",
       "  'MultinomialNB with TfidfTransformer',\n",
       "  'deep nueral network with tfidf',\n",
       "  'Embedding'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word',\n",
       "  'removing digits',\n",
       "  'removing non characters',\n",
       "  'removing emojies',\n",
       "  'removing stop word',\n",
       "  'character standardization',\n",
       "  'after stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'None',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming']}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"Embedding\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing deprecated letters from a word,removing digits,stemming\")\n",
    "model_comparison_table['accuracy'].append(accuracy)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMi10HDzz8eX"
   },
   "source": [
    "## Add a GlobalMaxPool1D layer to the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wImb28hyU3-z",
    "outputId": "5e36ae48-10d9-4936-d841-9681b9540f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 50)           1699350   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 50)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,699,871\n",
      "Trainable params: 1,699,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDVFjd_hVD7d",
    "outputId": "7489ff47-89cc-4317-a37c-f563efbdea26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.6926 - accuracy: 0.5166 - val_loss: 0.6766 - val_accuracy: 0.6905\n",
      "Epoch 2/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.6704 - accuracy: 0.6397 - val_loss: 0.6617 - val_accuracy: 0.6367\n",
      "Epoch 3/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.5905 - accuracy: 0.7408 - val_loss: 0.6738 - val_accuracy: 0.5889\n",
      "Epoch 4/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.4481 - accuracy: 0.8197 - val_loss: 0.7430 - val_accuracy: 0.5664\n",
      "Epoch 5/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.3142 - accuracy: 0.8911 - val_loss: 0.8182 - val_accuracy: 0.5756\n",
      "Epoch 6/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.2166 - accuracy: 0.9294 - val_loss: 0.9031 - val_accuracy: 0.5793\n",
      "Epoch 7/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.1543 - accuracy: 0.9520 - val_loss: 1.0411 - val_accuracy: 0.5731\n",
      "Epoch 8/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.1147 - accuracy: 0.9623 - val_loss: 1.2289 - val_accuracy: 0.5473\n",
      "Epoch 9/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0939 - accuracy: 0.9660 - val_loss: 1.2326 - val_accuracy: 0.5675\n",
      "Epoch 10/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0830 - accuracy: 0.9678 - val_loss: 1.3233 - val_accuracy: 0.5616\n",
      "Epoch 11/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0733 - accuracy: 0.9696 - val_loss: 1.3651 - val_accuracy: 0.5672\n",
      "Epoch 12/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.0709 - accuracy: 0.9696 - val_loss: 1.4648 - val_accuracy: 0.5580\n",
      "Epoch 13/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0652 - accuracy: 0.9716 - val_loss: 1.4474 - val_accuracy: 0.5694\n",
      "Epoch 14/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0622 - accuracy: 0.9727 - val_loss: 1.4634 - val_accuracy: 0.5701\n",
      "Epoch 15/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.0594 - accuracy: 0.9716 - val_loss: 1.4972 - val_accuracy: 0.5679\n",
      "Epoch 16/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0590 - accuracy: 0.9734 - val_loss: 1.4151 - val_accuracy: 0.5907\n",
      "Epoch 17/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0562 - accuracy: 0.9745 - val_loss: 1.6331 - val_accuracy: 0.5572\n",
      "Epoch 18/30\n",
      "279/279 [==============================] - 5s 19ms/step - loss: 0.0529 - accuracy: 0.9754 - val_loss: 1.5688 - val_accuracy: 0.5701\n",
      "Epoch 19/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.0544 - accuracy: 0.9723 - val_loss: 1.5510 - val_accuracy: 0.5760\n",
      "Epoch 20/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.0528 - accuracy: 0.9750 - val_loss: 1.7662 - val_accuracy: 0.5517\n",
      "Epoch 21/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0527 - accuracy: 0.9749 - val_loss: 1.6265 - val_accuracy: 0.5705\n",
      "Epoch 22/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0508 - accuracy: 0.9759 - val_loss: 1.6737 - val_accuracy: 0.5653\n",
      "Epoch 23/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0505 - accuracy: 0.9765 - val_loss: 1.6046 - val_accuracy: 0.5804\n",
      "Epoch 24/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.0488 - accuracy: 0.9770 - val_loss: 1.6885 - val_accuracy: 0.5701\n",
      "Epoch 25/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.0481 - accuracy: 0.9763 - val_loss: 1.6187 - val_accuracy: 0.5819\n",
      "Epoch 26/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.0463 - accuracy: 0.9777 - val_loss: 1.5818 - val_accuracy: 0.5870\n",
      "Epoch 27/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0480 - accuracy: 0.9757 - val_loss: 1.7623 - val_accuracy: 0.5646\n",
      "Epoch 28/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0460 - accuracy: 0.9777 - val_loss: 1.6681 - val_accuracy: 0.5764\n",
      "Epoch 29/30\n",
      "279/279 [==============================] - 5s 20ms/step - loss: 0.0458 - accuracy: 0.9766 - val_loss: 1.8186 - val_accuracy: 0.5609\n",
      "Epoch 30/30\n",
      "279/279 [==============================] - 6s 20ms/step - loss: 0.0448 - accuracy: 0.9774 - val_loss: 1.7443 - val_accuracy: 0.5675\n",
      "Training Accuracy: 0.9842\n",
      "Testing Accuracy:  0.5675\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=30,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=20)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "X3eJv2BEAwLd"
   },
   "outputs": [],
   "source": [
    "model_comparison_table['model_name'].append(\"Embedding with maxbooling\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing deprecated letters from a word,removing digits,stemming\")\n",
    "model_comparison_table['accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DImMVtZz0HRX"
   },
   "source": [
    "## Add a CNN layer to the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DW0-MYSCWMvO",
    "outputId": "da1d3e5c-8437-4e38-cfc6-528a717d3869"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 100)          3398700   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 96, 128)           64128     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,464,129\n",
      "Trainable params: 3,464,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xg7nWUa9Zeix",
    "outputId": "090d8c91-ec05-4c97-c651-8321c8d0d6b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "279/279 [==============================] - 18s 58ms/step - loss: 0.6934 - accuracy: 0.5019 - val_loss: 0.6957 - val_accuracy: 0.3485\n",
      "Epoch 2/30\n",
      "279/279 [==============================] - 16s 57ms/step - loss: 0.6773 - accuracy: 0.5847 - val_loss: 0.6474 - val_accuracy: 0.6272\n",
      "Epoch 3/30\n",
      "279/279 [==============================] - 16s 56ms/step - loss: 0.5035 - accuracy: 0.7746 - val_loss: 0.7257 - val_accuracy: 0.5709\n",
      "Epoch 4/30\n",
      "279/279 [==============================] - 16s 56ms/step - loss: 0.2444 - accuracy: 0.9163 - val_loss: 0.8519 - val_accuracy: 0.5760\n",
      "Epoch 5/30\n",
      "279/279 [==============================] - 17s 60ms/step - loss: 0.1415 - accuracy: 0.9549 - val_loss: 0.9954 - val_accuracy: 0.5742\n",
      "Epoch 6/30\n",
      "279/279 [==============================] - 16s 57ms/step - loss: 0.1152 - accuracy: 0.9659 - val_loss: 1.0524 - val_accuracy: 0.5657\n",
      "Epoch 7/30\n",
      "279/279 [==============================] - 16s 59ms/step - loss: 0.0896 - accuracy: 0.9709 - val_loss: 1.0623 - val_accuracy: 0.5723\n",
      "Epoch 8/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0737 - accuracy: 0.9743 - val_loss: 1.1942 - val_accuracy: 0.5653\n",
      "Epoch 9/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0617 - accuracy: 0.9759 - val_loss: 1.2349 - val_accuracy: 0.5646\n",
      "Epoch 10/30\n",
      "279/279 [==============================] - 16s 57ms/step - loss: 0.0558 - accuracy: 0.9775 - val_loss: 1.2261 - val_accuracy: 0.5819\n",
      "Epoch 11/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0508 - accuracy: 0.9765 - val_loss: 1.3455 - val_accuracy: 0.5550\n",
      "Epoch 12/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0449 - accuracy: 0.9757 - val_loss: 1.3381 - val_accuracy: 0.5749\n",
      "Epoch 13/30\n",
      "279/279 [==============================] - 16s 57ms/step - loss: 0.0429 - accuracy: 0.9779 - val_loss: 1.3594 - val_accuracy: 0.5764\n",
      "Epoch 14/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0381 - accuracy: 0.9799 - val_loss: 1.3682 - val_accuracy: 0.5701\n",
      "Epoch 15/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0372 - accuracy: 0.9781 - val_loss: 1.5197 - val_accuracy: 0.5653\n",
      "Epoch 16/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0346 - accuracy: 0.9792 - val_loss: 1.4732 - val_accuracy: 0.5845\n",
      "Epoch 17/30\n",
      "279/279 [==============================] - 16s 57ms/step - loss: 0.0382 - accuracy: 0.9786 - val_loss: 1.6546 - val_accuracy: 0.5624\n",
      "Epoch 18/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0314 - accuracy: 0.9799 - val_loss: 1.6744 - val_accuracy: 0.5410\n",
      "Epoch 19/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0310 - accuracy: 0.9811 - val_loss: 1.8024 - val_accuracy: 0.5318\n",
      "Epoch 20/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0322 - accuracy: 0.9829 - val_loss: 1.6616 - val_accuracy: 0.5815\n",
      "Epoch 21/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0348 - accuracy: 0.9797 - val_loss: 2.1089 - val_accuracy: 0.5116\n",
      "Epoch 22/30\n",
      "279/279 [==============================] - 16s 57ms/step - loss: 0.0319 - accuracy: 0.9819 - val_loss: 1.9080 - val_accuracy: 0.5484\n",
      "Epoch 23/30\n",
      "279/279 [==============================] - 16s 57ms/step - loss: 0.0293 - accuracy: 0.9819 - val_loss: 1.7719 - val_accuracy: 0.5841\n",
      "Epoch 24/30\n",
      "279/279 [==============================] - 16s 57ms/step - loss: 0.0270 - accuracy: 0.9828 - val_loss: 1.9559 - val_accuracy: 0.5690\n",
      "Epoch 25/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0300 - accuracy: 0.9815 - val_loss: 2.0437 - val_accuracy: 0.5440\n",
      "Epoch 26/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0290 - accuracy: 0.9826 - val_loss: 2.3125 - val_accuracy: 0.5374\n",
      "Epoch 27/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0332 - accuracy: 0.9815 - val_loss: 1.9849 - val_accuracy: 0.5834\n",
      "Epoch 28/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0375 - accuracy: 0.9811 - val_loss: 2.1783 - val_accuracy: 0.5771\n",
      "Epoch 29/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0338 - accuracy: 0.9801 - val_loss: 2.2345 - val_accuracy: 0.5572\n",
      "Epoch 30/30\n",
      "279/279 [==============================] - 16s 58ms/step - loss: 0.0307 - accuracy: 0.9824 - val_loss: 2.8510 - val_accuracy: 0.5061\n",
      "Training Accuracy: 0.9851\n",
      "Testing Accuracy:  0.5061\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=30,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=20)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjpA12glAC8G",
    "outputId": "300aa198-3128-4b81-9ee7-26c0ad70a550"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.5804195804195804,\n",
       "  0.5782112624217888,\n",
       "  0.5940375414059624,\n",
       "  0.5807876334192124,\n",
       "  0.5774751564225249,\n",
       "  0.5785793154214207,\n",
       "  0.4994479205005521,\n",
       "  0.5311004784688995,\n",
       "  0.5811556864188443,\n",
       "  0.5925653294074347,\n",
       "  0.6209054103790946,\n",
       "  0.6345233713654767,\n",
       "  0.5944055914878845,\n",
       "  0.559808611869812,\n",
       "  0.5675377249717712,\n",
       "  0.5060728788375854],\n",
       " 'model_name': ['logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'logistic regression with bag of words',\n",
       "  'Logistic regression',\n",
       "  'tfidf with logistic regression',\n",
       "  'MultinomialNB with TfidfTransformer',\n",
       "  'deep nueral network with tfidf',\n",
       "  'Embedding',\n",
       "  'Embedding with maxbooling',\n",
       "  'CNN with Embedding'],\n",
       " 'preprocessing_methods': ['none',\n",
       "  'removing mentions and urls',\n",
       "  'removing deprecated letters from a word',\n",
       "  'removing digits',\n",
       "  'removing non characters',\n",
       "  'removing emojies',\n",
       "  'removing stop word',\n",
       "  'character standardization',\n",
       "  'after stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'None',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming',\n",
       "  'removing deprecated letters from a word,removing digits,stemming']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparison_table['model_name'].append(\"CNN with Embedding\")\n",
    "model_comparison_table['preprocessing_methods'].append(\"removing deprecated letters from a word,removing digits,stemming\")\n",
    "model_comparison_table['accuracy'].append(accuracy)\n",
    "model_comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gETqKxKfAxtt"
   },
   "source": [
    "## [3.3] Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "3RjC6u1WAxtv",
    "outputId": "865f4776-4031-4098-bda7-e0fa27b3fb99"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-133b0b26-1652-4af5-9936-c122afbcba49\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>preprocessing_methods</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>none</td>\n",
       "      <td>0.580420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing mentions and urls</td>\n",
       "      <td>0.578211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing deprecated letters from a word</td>\n",
       "      <td>0.594038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing digits</td>\n",
       "      <td>0.580788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing non characters</td>\n",
       "      <td>0.577475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing emojies</td>\n",
       "      <td>0.578579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>removing stop word</td>\n",
       "      <td>0.499448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>character standardization</td>\n",
       "      <td>0.531100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>logistic regression with bag of words</td>\n",
       "      <td>after stemming</td>\n",
       "      <td>0.581156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>removing deprecated letters from a word,removi...</td>\n",
       "      <td>0.592565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tfidf with logistic regression</td>\n",
       "      <td>removing deprecated letters from a word,removi...</td>\n",
       "      <td>0.620905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MultinomialNB with TfidfTransformer</td>\n",
       "      <td>None</td>\n",
       "      <td>0.634523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>deep nueral network with tfidf</td>\n",
       "      <td>removing deprecated letters from a word,removi...</td>\n",
       "      <td>0.594406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Embedding</td>\n",
       "      <td>removing deprecated letters from a word,removi...</td>\n",
       "      <td>0.559809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Embedding with maxbooling</td>\n",
       "      <td>removing deprecated letters from a word,removi...</td>\n",
       "      <td>0.567538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CNN with Embedding</td>\n",
       "      <td>removing deprecated letters from a word,removi...</td>\n",
       "      <td>0.506073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-133b0b26-1652-4af5-9936-c122afbcba49')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-133b0b26-1652-4af5-9936-c122afbcba49 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-133b0b26-1652-4af5-9936-c122afbcba49');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                               model_name  ...  accuracy\n",
       "0   logistic regression with bag of words  ...  0.580420\n",
       "1   logistic regression with bag of words  ...  0.578211\n",
       "2   logistic regression with bag of words  ...  0.594038\n",
       "3   logistic regression with bag of words  ...  0.580788\n",
       "4   logistic regression with bag of words  ...  0.577475\n",
       "5   logistic regression with bag of words  ...  0.578579\n",
       "6   logistic regression with bag of words  ...  0.499448\n",
       "7   logistic regression with bag of words  ...  0.531100\n",
       "8   logistic regression with bag of words  ...  0.581156\n",
       "9                     Logistic regression  ...  0.592565\n",
       "10         tfidf with logistic regression  ...  0.620905\n",
       "11    MultinomialNB with TfidfTransformer  ...  0.634523\n",
       "12         deep nueral network with tfidf  ...  0.594406\n",
       "13                              Embedding  ...  0.559809\n",
       "14              Embedding with maxbooling  ...  0.567538\n",
       "15                     CNN with Embedding  ...  0.506073\n",
       "\n",
       "[16 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(model_comparison_table)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhXdmEZR7vy6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP HOMEWORK - PART 5 BY [your_name].ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
